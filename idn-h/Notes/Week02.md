# Optimization Algorithms
Learning Objectives

- Remember different optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam
- Use random minibatches to accelerate the convergence and improve the optimization
- Know the benefits of learning rate decay and apply it to your optimization

## Mini-batch Gradient Descent

## Understanding mini-batch Gradient Descent

## Exponentially Weighted Averages

## Understanding Exponentially Weighted Averages

## Bias Correction in Exponentially Weighted Averages

## Gradient Descent with Momentum

## RMSProp

## Adam Optimization Algorithm

## Learning Rate Decay

## The Problem of Local Optima
  
