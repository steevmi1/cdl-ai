# Practical Aspects of Deep Learning
## Learning Objectives
Recall that different types of initializations lead to different results

Recognize the importance of initialization in complex neural networks.

Recognize the difference between train/dev/test sets

Diagnose the bias and variance issues in your model

Learn when and how to use regularization methods such as dropout or L2 regularization.

Understand experimental issues in deep learning such as Vanishing or Exploding gradients and learn how to deal with them

Use gradient checking to verify the correctness of your backpropagation implementation

# Setting up your machine learning application

## train/dev/test sets

## Bias/Variance

## Basic Recipe for Machine Learning

# Regularizing your neural network

## Regularization

## Why Regularization Reduces Overfitting

## Dropout Regularization

## Understanding Dropout

## Other Regularization Methods

# Setting up your optimization problem

## Normalizing inputs

## Vanishing/Exploding Gradients

## Weight Initialization for Deep Networks

## Numerical Approximations of Gradients

## Gradient Checking

## Gradient Checking Implementation Notes
